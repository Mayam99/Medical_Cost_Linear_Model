# -*- coding: utf-8 -*-
"""Medical_Cost.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OFSp6wgcgSWWi82gMQP19cBDWXm64hV_

## Step 1: Understand the Problem and Data

First, we need to understand what we're trying to predict. Typically with insurance datasets, we predict medical charges based on various factors.

Common variables in insurance datasets:

* Age

* Sex

* BMI

* Children (number of dependents)

* Smoker (yes/no)

* Region

* Charges (medical costs - this is typically our target)

##Step 2: Load and Explore the Data
"""

# Importing Necessary Libraries
import pandas as pd # Data Manipulation
import numpy as np # Numerical Computations
import matplotlib.pyplot as plt # Plotting
import seaborn as sns # Statistical Graphics

df = pd.read_csv('insurance.csv') # Reading the data from the CSV file named 'insurance.csv' and stores it in a pandas DataFrame named df.

df.head() # Displays the first 5 rows of the DataFrame df, allowing you to quickly inspect the data.

df.info()  # Prints a concise summary of the DataFrame df, including data types, non-null values, and memory usage.

df.describe() # Generates descriptive statistics of the DataFrame's numerical columns, like mean, standard deviation, and quartiles

"""##Step 3: Data Cleaning and Preprocessing"""

df.isnull().sum() # Calculates and displays the number of missing (null) values in each column of the DataFrame df.

"""There are no missing values in the data set"""

df.duplicated().sum() # Calculates and displays the total number of duplicate rows in the DataFrame df.

"""There 1 duplicated value"""

df.duplicated() # Returns a boolean Series indicating whether each row in the DataFrame df is a duplicate of a previous row.

# It converts categorical columns ('sex', 'smoker', 'region') into numerical using one-hot encoding and removes the first category to avoid redundancy.
df = pd.get_dummies(df, columns=['sex', 'smoker', 'region'], drop_first=True)

df.head() # Checking the data frame.

"""## Step 4: Exploratory Data Analysis (EDA)"""

# Visualize Distribution
plt.figure(figsize=(12,6)) # size unit is in 'Inches'.
sns.histplot(df['charges'], kde=True)
plt.title('Distribution of Medical Charges')
plt.show()

"""* Positively Skewed: The histogram is positively skewed or right-skewed, meaning the tail of the distribution extends longer to the right. This indicates that most people have lower medical charges, while a smaller number of people have very high charges, pulling the average towards the higher end.

* Most Common Charges: The highest bars in the histogram represent the most frequent charge amounts. In this case, it appears that charges in the lower range (perhaps around $1000- $2000) are most common.

* Outliers: There are a few bars towards the far right of the histogram that are relatively short. These represent outliers - unusually high medical charges that are much higher than the majority of the data.
"""

# Correlation Matrix
plt.figure(figsize=(8,6))# size unit is in 'Inches'.
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

"""The heatmap shows how strongly different factors in the insurance dataset are related to each other. Darker red colors mean a stronger positive relationship (as one goes up, the other tends to go up), darker blue colors mean a stronger negative relationship (as one goes up, the other tends to go down), and lighter colors mean a weaker relationship.

For example, you'll likely see a strong positive relationship between 'smoker' and 'charges', indicating that smokers tend to have higher medical charges. You might also see some relationship between 'age' and 'charges', or 'bmi' and 'charges'.

Essentially, it gives you a visual way to identify the factors that are most likely to influence medical charges based on the data.
"""

# Pair Plot for Numerical Variables
sns.pairplot(df[['age', 'bmi', 'children', 'charges']])
plt.show()

"""## Step 5: Feature Engineering"""

df['age_squared'] = df['age']**2 # It creates a new column called 'age_squared' in the DataFrame df and calculates it by squaring the values in the 'age' column.
df['bmi_category'] = pd.cut(df['bmi'],
                            bins=[0, 18.5, 25, 30, 100],
                            labels=['underweight', 'normal', 'overweight', 'obese'])
# It creates a new column called 'bmi_category' and categorizes individuals into 'underweight', 'normal', 'overweight', or 'obese' based on their BMI values using specified ranges (bins).


# Convert new categorical features
df = pd.get_dummies(df, columns=['bmi_category'], drop_first=True)

"""## Step 6: Prepare Data for Modeling"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Defining features and target
X = df.drop('charges', axis=1)
y = df['charges']

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale numerical features
scaler = StandardScaler()
num_cols = ['age', 'bmi', 'children', 'age_squared']
X_train[num_cols] = scaler.fit_transform(X_train[num_cols])
X_test[num_cols] = scaler.transform(X_test[num_cols])

"""## Step 7: Build and Train the Linear Regression Model"""

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# Initialize and train model
lr = LinearRegression()
lr.fit(X_train, y_train)

# Make predictions
y_pred = lr.predict(X_test)

"""## Step 8: Model Evaluation"""

# Calculate metrics
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse:.2f}")
print(f"Root Mean Squared Error: {rmse:.2f}")
print(f"Mean Absolute Error: {mae:.2f}")
print(f"R-squared: {r2:.2f}")

"""* Mean Squared Error (MSE): Measures the average squared difference between the predicted and actual values. It gives higher weight to larger errors.

* Root Mean Squared Error (RMSE): The square root of MSE, providing a more interpretable metric in the original units of the target variable (medical charges).

* Mean Absolute Error (MAE): Measures the average absolute difference between the predicted and actual values. It treats all errors equally.

* R-squared: Represents the proportion of variance in the target variable (medical charges) explained by the model. It indicates how well the model fits the data, with higher values indicating a better fit.
"""

# Residual plot
residuals = y_test - y_pred
plt.figure(figsize=(10, 6))
sns.scatterplot(x=y_pred, y=residuals)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
plt.title('Residual Plot')
plt.show()

"""The residual plot is a scatter plot that shows the residuals (the difference between the predicted values and the actual values) on the y-axis and the predicted values on the x-axis. It's a crucial diagnostic tool for assessing the assumptions of a linear regression model.

Ideal Pattern:

In an ideal scenario, the residuals should be randomly scattered around the horizontal line at 0. This indicates that the model's errors are random and there's no systematic pattern in the errors.

Interpreting the Plot:

Randomness: If the residuals appear to be randomly scattered around the horizontal line, it suggests that the linear regression model is a good fit for the data.

Patterns: If there are any clear patterns in the residuals (e.g., a curve, a funnel shape), it suggests that the linear regression assumptions might be violated, and the model may not be the best fit.

Outliers: Points that are far away from the horizontal line may be outliers, which can have a significant influence on the regression model.
"""